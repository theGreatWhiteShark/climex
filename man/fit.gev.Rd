% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/opti.R
\name{fit.gev}
\alias{fit.gev}
\title{Improved maximum-likelihood fit of the GEV distribution}
\usage{
fit.gev(x, initial = NULL, likelihood.function = likelihood,
  gradient.function = likelihood.gradient, error.estimation = c("MLE", "MC",
  "bootstrap", "none"), monte.carlo.sample.size = 100,
  bootstrap.sample.size = 100, return.period = 100,
  extreme.type = c("max", "min"), blocking = FALSE, block.number = NULL,
  block.length = NULL, silent = TRUE, mc.cores = NULL, ...)
}
\arguments{
\item{x}{Either an object of class \pkg{xts} or a list of
those. The time series can be provided in two different formats:
as a series of block maxima (possibly calculated using the
\code{\link{block}} function) and the \strong{blocking} argument
set to FALSE or as the raw time series with \strong{blocking}
set to TRUE. In the later case the blocking will be performed
inside the fit.gev function.}

\item{initial}{Initial values for the GEV parameters. Has to be
provided as 3x1 vector. If NULL the parameters are estimated
using \code{\link{likelihood.initials}}. If the shape parameter
is set to 0 the exponential distribution instead of the GP one
is fitted. But this its strongly discouraged to do so! Default =
NULL.}

\item{likelihood.function}{Function, which is going to be
optimized. Default: \code{\link{likelihood}}}

\item{gradient.function}{If NULL a finite difference method is
invoked. Default: \code{\link{likelihood.gradient}}}

\item{error.estimation}{Method for calculating the standard errors
  of the fitted results. The errors of the GEV parameters will be
  calculated as the square roots of the diagonal elements of the
  inverse of the hessian matrix. The latter will be evaluated at
  the maximum likelihood estimates (MLE) of the GEV parameters.

  \emph{MLE}: The standard error of the return level is calculated
    using the Delta method and the maximum likelihood estimates of
    the GPD parameters. Note: For positive shape parameters bigger
    than 0.3 this approach tends to highly overestimates the
    errors of the return levels. 

  \emph{MC}: Alternative one can use a Monte Carlo method for
    which \strong{monte.carlo.sample.size} samples of the same
    size as \strong{x} will be drawn from a GEV distribution
    constituted by the obtained MLE of the GEV parameters of
    \strong{x}. The standard error is then calculated via the
    square of the variance of all fitted GEV parameters and
    calculated return levels. Note: In its essence this approach
    is not an estimation of the error involved in fitting the time
    series to a GEV distribution. It is rather the mean error of
    fitting a GPD-distribution with the same length and parameters
    as estimated ones.

  \emph{bootstrap}: Using this option the provided time series
    \strong{x} will be sampled with replacement
    \strong{bootstrap.sample.size} times and with the same length
    as the original time series. The standard errors of the GEV
    parameters and return levels of all those sampled series is
    calculated and returned as an estimate of the fitting error.
    Note: Since the data is (hopefully) GEV-distributed, such a
    sampling has to be treated with a lot of care.

    Sometimes the inversion of the hessian fails (since the are
    some NaN in the hessian) when calculating the error estimates
    using the maximum likelihood approach (MLE) (which is also the
    reason why the \pkg{ismev} package occasionally does not
    work). In such cases the Monte Carlo (MC) method is used as a
    fallback.

  \emph{none} skips the calculation of the error. Default = "MLE".}

\item{monte.carlo.sample.size}{Number of samples used to obtain
the Monte Carlo estimate of the standard error of the fitting.
Default = 100.}

\item{bootstrap.sample.size}{Number of samples with replacements
to drawn from the original series \strong{x} in order to
determine the standard errors for the GPD parameters and return
levels. Default = 100.}

\item{return.period}{Quantiles at which the return level is going
to be evaluated. Class "numeric". Default = 100.}

\item{extreme.type}{String specifying whether the maxima ("max")
or minima ("min") of each block should be fitted. If the minima
are chosen, the input \strong{x} has to be either a time series
of block minima with the 
\strong{blocking} argument set to zero or the raw series and
fit.gev function will handle the extraction of the minima
internally. For the minima the resulting extremes are multiplied
by -1 and fitted using a default GEV distribution. The resulting
scale and shape parameter are handed back without any additional
changes and the location parameter and return levels are
multiplied by -1. Default = "max".}

\item{blocking}{Logical value indicating whether or not the input
data \strong{x} should be split into blocks of equal size using
the \code{\link{block}} function and only the maxima or minima
should be extracted. If any of the arguments
\strong{block.number} or \strong{block.length} is provided
with a non NULL value, the blocking will be performed regardless
of the value of the \strong{blocking} argument. If, on the other
hand, \strong{blocking} is TRUE but neither
\strong{block.number} nor \strong{block.length} was provided,
the time series will be split into annual blocks. Default =
FALSE.}

\item{block.number}{Specifies the number of blocks the input data
is going to be separated in. Default = NULL.}

\item{block.length}{Length of the blocks. For the sake of
simplicity the last block is not forced to match the length of
the other plots. Default = NULL.}

\item{silent}{Determines whether or not warning messages shall be
displayed and results shall be reported. Default = TRUE.}

\item{mc.cores}{A numerical input specifying the number of cores
to use for the multi core application of the function (see
\code{\link[parallel]{detectCores}}). This functionality is only
available if the input is a list of different objects. If NULL,
the function will be calculated classically with only one core.
Default = NULL.}

\item{...}{Additional arguments for the \code{\link[stats]{optim}}
function.}
}
\value{
Output of the optim function with class \code{c( "list",
  "climex.fit.gev" )} 
  \itemize{
    \item{ par = MLE of the GEV parameters }
    \item{ value = Value of the negative log-likelihood evaluated
             at the MLE }
    \item{ counts = Number of evaluations of the likelihood
             function and its gradient during optimization (inner
             routine) }  
    \item{ outer.iteration = Number of updates of the penalty and
             the Lagrangian parameter to fine-tune the impact of
             the constraints on the optimization (outer routine) }
    \item{ return.level = Estimate of the return levels at the
             provided return periods }
    \item{ se = Standard error of the GEV parameters and the
             return levels }
    \item{ x = Original time series }
    \item{ control = Parameter and options used during
             optimization } 
  }
  If, on the other hand, a list of \pkg{xts} class object was
  provided, a list of objects structured as describe above is
  returned.
}
\description{
This function fits the Generalized Extreme Value
  (GEV) distribution to the supplied data, which has to be
  composed of block maxima (preferably without trend and
  correlations). The determination of the starting point of the
  optimization and the calculation of the return level and the all
  the corresponding estimates of the fitting errors will be done
  internally.
}
\details{
The optimization is performed by the augmented Lagrangian 
  method using the \code{\link{auglag}} function of the
  \pkg{alabama} package. Within this framework the log-likelihood
  function of the GEV distribution gets augmented with N+2
  constraints, where N is the number of points in the time
  series. N+1 of those constraints ensure the log-likelihood
  (containing two logarithms) to be always defined. The remaining
  constraints ensures for the shape parameter to be always bigger
  than -1 for the maximum likelihood to be defined in the first
  place. The penalty in the log-likelihood function is the sum of
  all squared constrain violations plus an additional term linear
  in the constraint violation to ensure well-conditioning. Using
  this penalty term the problem becomes unconstrained again and
  can be solved using \code{\link[stats]{optim}}. After each of
  those inner routines the weighting parameter of the penalty is
  being increased until some convergence conditions are fulfilled.

  Since it usually takes just four to five outer iterations this
  functions needs only double the time a pure call to the
  \code{\link[stats]{optim}} function would need.

  The negative log-likelihood of the Gumbel distribution is just
  fitted if the shape parameter is exactly equal to zero.

  If the user instead wants to fit just the Gumbel distribution
  and not the entire GEV distribution, the shape parameter of the
  \strong{initial} has to be set to 0. But in practice this is
  strongly discouraged since it will yield inferior results.

  I found the Nelder-Mead method to be more robust to starting
  points more far away from the global optimum. This also holds 
  for the inner routine of the augmented Lagrangian method. Since
  other routines, like CG and BFGS only cause problems in the
  extreme value analysis, there won't be an option to choose them
  in this package.

  This function can also be applied to a list of \pkg{xts} class
  objects.
}
\examples{
  potsdam.anomalies <- anomalies( temp.potsdam )
  potsdam.blocked <- block( potsdam.anomalies )
  fit.gev( potsdam.blocked )
}
\seealso{
Other optimization: \code{\link{fit.gev.list}},
  \code{\link{fit.gev.xts}}, \code{\link{fit.gpd.list}},
  \code{\link{fit.gpd.xts}}, \code{\link{fit.gpd}},
  \code{\link{likelihood.augmented}},
  \code{\link{likelihood.gradient.augmented}},
  \code{\link{likelihood.gradient}},
  \code{\link{likelihood.initials}},
  \code{\link{likelihood}}
}
\author{
Philipp Mueller
}
